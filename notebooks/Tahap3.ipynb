{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "4wYkjjpkk7sD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "blWiEp3_jKVw"
      },
      "outputs": [],
      "source": [
        "# === TAHAP 3: CASE RETRIEVAL ===\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import json\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# === Load Data ===\n",
        "df = pd.read_csv('/content/drive/MyDrive/ProyekA/data/processed/cases.csv')\n",
        "df['ringkasan_fakta'] = df['ringkasan_fakta'].fillna('')\n",
        "\n",
        "# === TF-IDF + SVM ===\n",
        "vectorizer = TfidfVectorizer()\n",
        "X_tfidf = vectorizer.fit_transform(df['ringkasan_fakta'])\n",
        "y = df['pasal'].astype(str)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_tfidf, y, test_size=0.3, random_state=42)\n",
        "\n",
        "clf = SVC(kernel='linear')\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "y_pred = clf.predict(X_test)\n",
        "print(\"=== TF-IDF + SVM Evaluation ===\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "# === IndoBERT Embedding ===\n",
        "!pip install -q transformers sentencepiece\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "import torch\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"indobenchmark/indobert-base-p1\")\n",
        "model = AutoModel.from_pretrained(\"indobenchmark/indobert-base-p1\")\n",
        "\n",
        "def bert_embed(text):\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=512)\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "    return outputs.last_hidden_state[:, 0, :].numpy()\n",
        "\n",
        "# Hitung BERT embedding sekali saja\n",
        "df['embedding'] = df['ringkasan_fakta'].apply(lambda x: bert_embed(x)[0])\n",
        "X_embed = np.vstack(df['embedding'].values)\n",
        "\n",
        "# === Fungsi retrieve(query, k, mode) ===\n",
        "def retrieve(query: str, k: int = 5, mode: str = 'tfidf') -> list:\n",
        "    if mode == 'bert':\n",
        "        q_vec = bert_embed(query)  # ✅ gunakan BERT\n",
        "        sims = cosine_similarity(q_vec, X_embed)[0]\n",
        "    else:\n",
        "        q_vec = vectorizer.transform([query])  # ✅ gunakan TF-IDF\n",
        "        sims = cosine_similarity(q_vec, X_tfidf).flatten()\n",
        "\n",
        "    topk_idx = sims.argsort()[-k:][::-1]\n",
        "    return df.iloc[topk_idx]['case_id'].astype(str).tolist()\n",
        "\n",
        "# === Buat Query Uji & Simpan queries.json ===\n",
        "eval_queries = [\n",
        "    {\n",
        "        \"query_id\": \"q001\",\n",
        "        \"query_text\": \"Terdakwa membawa sabu seberat 3 gram dalam plastik kecil\",\n",
        "        \"ground_truth\": \"001\"\n",
        "    },\n",
        "    {\n",
        "        \"query_id\": \"q002\",\n",
        "        \"query_text\": \"Tersangka mengedarkan ekstasi di tempat hiburan malam\",\n",
        "        \"ground_truth\": \"002\"\n",
        "    },\n",
        "    {\n",
        "        \"query_id\": \"q003\",\n",
        "        \"query_text\": \"Polisi menangkap pelaku narkoba saat razia di terminal\",\n",
        "        \"ground_truth\": \"003\"\n",
        "    },\n",
        "    {\n",
        "        \"query_id\": \"q004\",\n",
        "        \"query_text\": \"Petugas menemukan ganja di dalam jok motor terdakwa\",\n",
        "        \"ground_truth\": \"004\"\n",
        "    },\n",
        "    {\n",
        "        \"query_id\": \"q005\",\n",
        "        \"query_text\": \"Tersangka menyimpan sabu di laci rumahnya untuk dijual\",\n",
        "        \"ground_truth\": \"005\"\n",
        "    }\n",
        "]\n",
        "\n",
        "# Simpan ke /data/eval/queries.json\n",
        "eval_path = \"/content/drive/MyDrive/ProyekA/data/eval\"\n",
        "os.makedirs(eval_path, exist_ok=True)\n",
        "with open(os.path.join(eval_path, \"queries.json\"), \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(eval_queries, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "print(\"✓ File queries.json berhasil disimpan.\")\n"
      ]
    }
  ]
}