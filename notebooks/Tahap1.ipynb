{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "TAHAP 1"
      ],
      "metadata": {
        "id": "JYPVmxy1YM-l"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "53MYaqZ90k1t",
        "outputId": "70a67e6d-c2c5-42cf-9f11-bc4b1b5bd906"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pandas requests beautifulsoup4 pdfminer.six lxml > /dev/null 2>&1"
      ],
      "metadata": {
        "id": "cy30KYpG04YY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "import pandas as pd\n",
        "import glob\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.metrics import classification_report, accuracy_score, precision_score, recall_score, f1_score\n",
        "from collections import Counter\n",
        "import json\n",
        "import matplotlib.pyplot as plt\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "import torch"
      ],
      "metadata": {
        "id": "DOR4uvlc1DH9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# PERSIAPAN FOLDER DI GOOGLE DRIVE\n",
        "raw_dir = '/content/drive/MyDrive/ProyekA/data/raw'\n",
        "logs_dir = '/content/drive/MyDrive/ProyekA/logs'\n",
        "os.makedirs(raw_dir, exist_ok=True)\n",
        "os.makedirs(logs_dir, exist_ok=True)\n",
        "\n",
        "# FUNGSI HELPER\n",
        "def create_path(folder_name):\n",
        "    path = os.path.join(os.getcwd(), folder_name)\n",
        "    if not os.path.exists(path):\n",
        "        os.makedirs(path)\n",
        "    return path\n",
        "\n",
        "def open_page(link):\n",
        "    count = 0\n",
        "    while count < 3:\n",
        "        try:\n",
        "            return BeautifulSoup(requests.get(link).text, \"lxml\")\n",
        "        except:\n",
        "            count += 1\n",
        "            time.sleep(5)\n",
        "\n",
        "def get_detail(soup, keyword):\n",
        "    try:\n",
        "        text = (\n",
        "            soup.find(lambda tag: tag.name == \"td\" and keyword in tag.text)\n",
        "            .find_next()\n",
        "            .get_text()\n",
        "            .strip()\n",
        "        )\n",
        "        return text\n",
        "    except:\n",
        "        return \"\"\n",
        "\n",
        "def get_pdf(url, path_pdf):\n",
        "    try:\n",
        "        file = urllib.request.urlopen(url)\n",
        "        file_name = os.path.basename(url)\n",
        "        file_content = file.read()\n",
        "        with open(f\"{path_pdf}/{file_name}\", \"wb\") as out_file:\n",
        "            out_file.write(file_content)\n",
        "        return io.BytesIO(file_content), file_name\n",
        "    except:\n",
        "        return None, None\n",
        "\n",
        "def clean_text(text):\n",
        "    text = text.replace(\"M a h ka m a h A g u n g R e p u blik In d o n esia\\n\", \"\")\n",
        "    text = text.replace(\"Disclaimer\\n\", \"\")\n",
        "    text = text.replace(\n",
        "        \"Kepaniteraan Mahkamah Agung Republik Indonesia berusaha untuk selalu mencantumkan informasi paling kini dan akurat sebagai bentuk komitmen Mahkamah Agung untuk pelayanan publik, transparansi dan akuntabilitas\\n\",\n",
        "        \"\",\n",
        "    )\n",
        "    text = text.replace(\n",
        "        \"pelaksanaan fungsi peradilan. Namun dalam hal-hal tertentu masih dimungkinkan terjadi permasalahan teknis terkait dengan akurasi dan keterkinian informasi yang kami sajikan, hal mana akan terus kami perbaiki dari waktu kewaktu.\\n\",\n",
        "        \"\",\n",
        "    )\n",
        "    text = text.replace(\n",
        "        \"Dalam hal Anda menemukan inakurasi informasi yang termuat pada situs ini atau informasi yang seharusnya ada, namun belum tersedia, maka harap segera hubungi Kepaniteraan Mahkamah Agung RI melalui :\\n\",\n",
        "        \"\",\n",
        "    )\n",
        "    text = text.replace(\n",
        "        \"Email : kepaniteraan@mahkamahagung.go.id    Telp : 021-384 3348 (ext.318)\\n\",\n",
        "        \"\",\n",
        "    )\n",
        "    return text\n",
        "\n",
        "# FUNGSI UTAMA UNTUK EKSTRAKSI DATA\n",
        "def extract_data(link, keyword_url, path_output, path_pdf, today):\n",
        "    soup = open_page(link)\n",
        "    table = soup.find(\"table\", {\"class\": \"table\"})\n",
        "    judul = table.find(\"h2\").text if table.find(\"h2\") else \"\"\n",
        "\n",
        "    nomor = get_detail(table, \"Nomor\")\n",
        "    tingkat_proses = get_detail(table, \"Tingkat Proses\")\n",
        "    klasifikasi = get_detail(table, \"Klasifikasi\")\n",
        "    kata_kunci = get_detail(table, \"Kata Kunci\")\n",
        "    tahun = get_detail(table, \"Tahun\")\n",
        "    tanggal_register = get_detail(table, \"Tanggal Register\")\n",
        "    lembaga_peradilan = get_detail(table, \"Lembaga Peradilan\")\n",
        "    jenis_lembaga_peradilan = get_detail(table, \"Jenis Lembaga Peradilan\")\n",
        "    hakim_ketua = get_detail(table, \"Hakim Ketua\")\n",
        "    hakim_anggota = get_detail(table, \"Hakim Anggota\")\n",
        "    panitera = get_detail(table, \"Panitera\")\n",
        "    amar = get_detail(table, \"Amar\")\n",
        "    amar_lainnya = get_detail(table, \"Amar Lainnya\")\n",
        "    catatan_amar = get_detail(table, \"Catatan Amar\")\n",
        "    tanggal_musyawarah = get_detail(table, \"Tanggal Musyawarah\")\n",
        "    tanggal_dibacakan = get_detail(table, \"Tanggal Dibacakan\")\n",
        "    kaidah = get_detail(table, \"Kaidah\")\n",
        "    status = get_detail(table, \"Status\")\n",
        "    abstrak = get_detail(table, \"Abstrak\")\n",
        "\n",
        "    try:\n",
        "        link_pdf = soup.find(\"a\", href=re.compile(r\"/pdf/\"))[\"href\"]\n",
        "        file_pdf, file_name_pdf = get_pdf(link_pdf, path_pdf)\n",
        "        text_pdf = extract_text(file_pdf)\n",
        "        text_pdf = clean_text(text_pdf)\n",
        "    except:\n",
        "        link_pdf = \"\"\n",
        "        text_pdf = \"\"\n",
        "        file_name_pdf = \"\"\n",
        "\n",
        "    #Simpan .txt RAW\n",
        "    raw_filename = re.sub(r'[^a-zA-Z0-9]', '_', judul)[:100] or 'no_title'\n",
        "    raw_file_path = os.path.join(raw_dir, f\"{raw_filename}.txt\")\n",
        "    raw_text = judul + '\\n' + (text_pdf or \"\")\n",
        "    with open(raw_file_path, 'w', encoding='utf-8') as raw_file:\n",
        "        raw_file.write(raw_text)\n",
        "\n",
        "    #LOG CLEANING & PANJANG TEKS\n",
        "    panjang_asli = len(raw_text)\n",
        "    panjang_clean = len(clean_text(raw_text))\n",
        "    status_validasi = \"VALID\" if panjang_clean >= 0.8 * panjang_asli else \"INVALID\"\n",
        "\n",
        "    with open(f\"{logs_dir}/cleaning.log\", \"a\") as log_file:\n",
        "        log_file.write(\n",
        "            f\"{raw_filename}.txt | panjang_asli: {panjang_asli}, panjang_clean: {panjang_clean}, status: {status_validasi}\\n\"\n",
        "        )\n",
        "\n",
        "    #SIMPAN KE CSV\n",
        "    data = [\n",
        "        judul,\n",
        "        nomor,\n",
        "        tingkat_proses,\n",
        "        klasifikasi,\n",
        "        kata_kunci,\n",
        "        tahun,\n",
        "        tanggal_register,\n",
        "        lembaga_peradilan,\n",
        "        jenis_lembaga_peradilan,\n",
        "        hakim_ketua,\n",
        "        hakim_anggota,\n",
        "        panitera,\n",
        "        amar,\n",
        "        amar_lainnya,\n",
        "        catatan_amar,\n",
        "        tanggal_musyawarah,\n",
        "        tanggal_dibacakan,\n",
        "        kaidah,\n",
        "        status,\n",
        "        abstrak,\n",
        "        link,\n",
        "        link_pdf,\n",
        "        file_name_pdf,\n",
        "        text_pdf,\n",
        "    ]\n",
        "    result = pd.DataFrame(\n",
        "        [data],\n",
        "        columns=[\n",
        "            \"judul\",\n",
        "            \"nomor\",\n",
        "            \"tingkat_proses\",\n",
        "            \"klasifikasi\",\n",
        "            \"kata_kunci\",\n",
        "            \"tahun\",\n",
        "            \"tanggal_register\",\n",
        "            \"lembaga_peradilan\",\n",
        "            \"jenis_lembaga_peradilan\",\n",
        "            \"hakim_ketua\",\n",
        "            \"hakim_anggota\",\n",
        "            \"panitera\",\n",
        "            \"amar\",\n",
        "            \"amar_lainnya\",\n",
        "            \"catatan_amar\",\n",
        "            \"tanggal_musyawarah\",\n",
        "            \"tanggal_dibacakan\",\n",
        "            \"kaidah\",\n",
        "            \"status\",\n",
        "            \"abstrak\",\n",
        "            \"link\",\n",
        "            \"link_pdf\",\n",
        "            \"file_name_pdf\",\n",
        "            \"text_pdf\",\n",
        "        ],\n",
        "    )\n",
        "\n",
        "    keyword_url = keyword_url.replace(\"/\", \" \")\n",
        "    if keyword_url.startswith(\"https\"):\n",
        "        keyword_url = \"\"\n",
        "    destination = f\"{path_output}/putusan_ma_{keyword_url}_{today}.csv\"\n",
        "\n",
        "    if not os.path.isfile(destination):\n",
        "        result.to_csv(destination, header=True, index=False)\n",
        "    else:\n",
        "        result.to_csv(destination, mode=\"a\", header=False, index=False)\n",
        "\n",
        "#MAIN FUNCTION\n",
        "def run_process(keyword_url, page, sort_date, path_output, path_pdf, today):\n",
        "    if keyword_url.startswith(\"https\"):\n",
        "        link = f\"{keyword_url}&page={page}\"\n",
        "    else:\n",
        "        link = f\"https://putusan3.mahkamahagung.go.id/search.html?q={keyword_url}&page={page}\"\n",
        "    if sort_date:\n",
        "        link = f\"{link}&obf=TANGGAL_PUTUS&obm=desc\"\n",
        "\n",
        "    soup = open_page(link)\n",
        "    links = soup.find_all(\"a\", {\"href\": re.compile(\"/direktori/putusan\")})\n",
        "    for link in links:\n",
        "        extract_data(link[\"href\"], keyword_url, path_output, path_pdf, today)\n",
        "\n",
        "def run_scraper(keyword=None, url=None, sort_date=True, download_pdf=True):\n",
        "    if not keyword and not url:\n",
        "        print(\"Please provide a keyword or URL.\")\n",
        "        return\n",
        "\n",
        "    path_output = '/content/drive/MyDrive/ProyekA/CSV'\n",
        "    path_pdf = '/content/drive/MyDrive/ProyekA/PDF'\n",
        "    today = date.today().strftime(\"%Y-%m-%d\")\n",
        "\n",
        "    link = f\"https://putusan3.mahkamahagung.go.id/search.html?q={keyword}&page=1\"\n",
        "    if url:\n",
        "        link = url\n",
        "\n",
        "    soup = open_page(link)\n",
        "    last_page = int(soup.find_all(\"a\", {\"class\": \"page-link\"})[-1].get(\"data-ci-pagination-page\"))\n",
        "\n",
        "    if url:\n",
        "        print(f\"Scraping with url: {url} - {20 * last_page} data - {last_page} page\")\n",
        "    else:\n",
        "        print(f\"Scraping with keyword: {keyword} - {20 * last_page} data - {last_page} page\")\n",
        "\n",
        "    keyword_url = url if url else keyword\n",
        "    futures = []\n",
        "    with ThreadPoolExecutor(max_workers=4) as executor:\n",
        "        for page in range(last_page):\n",
        "            futures.append(\n",
        "                executor.submit(run_process, keyword_url, page + 1, sort_date, path_output, path_pdf, today)\n",
        "            )\n",
        "    wait(futures)\n"
      ],
      "metadata": {
        "id": "HyO8Zpck1I42"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download Pidana PN MAKASSAR\n",
        "run_scraper(url=\"https://putusan3.mahkamahagung.go.id/search.html?q=Sulawesi&jenis_doc=putusan&tp=0&t_reg=2024&court=099422PN142&cat=bfa5809fc342e6a6ef5d3d9de5ec7075&jd=AMAR_LAINNYA\")"
      ],
      "metadata": {
        "id": "eMN4NB7O3Hir",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "867c59d8-4a31-4bb4-e302-a4659a186309"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scraping with url: https://putusan3.mahkamahagung.go.id/search.html?q=Sulawesi&jenis_doc=putusan&tp=0&t_reg=2024&court=099422PN142&cat=bfa5809fc342e6a6ef5d3d9de5ec7075&jd=AMAR_LAINNYA - 40 data - 2 page\n"
          ]
        }
      ]
    }
  ]
}