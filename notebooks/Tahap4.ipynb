{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "bQPw8rsIlD80"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-zdowK5_jJKY"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "from collections import Counter\n",
        "import os\n",
        "import json\n",
        "\n",
        "# === SETUP ===\n",
        "# Load data kasus\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/ProyekA/data/processed/cases.csv\")\n",
        "df['ringkasan_fakta'] = df['ringkasan_fakta'].fillna('')\n",
        "df['pasal'] = df['pasal'].fillna('UNKNOWN').astype(str).str.lower().str.strip().str[:100]\n",
        "\n",
        "# === FIX case_id agar konsisten ===\n",
        "df[\"case_id\"] = df.index + 1\n",
        "df[\"case_id\"] = df[\"case_id\"].astype(str).str.zfill(3)  # '001', '002', ...\n",
        "\n",
        "# === TF-IDF Vectorizer ===\n",
        "vectorizer = TfidfVectorizer()\n",
        "X_tfidf = vectorizer.fit_transform(df[\"ringkasan_fakta\"])\n",
        "\n",
        "# === IndoBERT Setup ===\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"indobenchmark/indobert-base-p1\")\n",
        "model = AutoModel.from_pretrained(\"indobenchmark/indobert-base-p1\")\n",
        "\n",
        "def bert_embed(text):\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=512)\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "    return outputs.last_hidden_state[:, 0, :].numpy()\n",
        "\n",
        "# === Precompute Embedding BERT ===\n",
        "print(\"⏳ Menghitung IndoBERT embeddings...\")\n",
        "df['embedding'] = df['ringkasan_fakta'].apply(lambda x: bert_embed(x)[0])\n",
        "X_embed = np.vstack(df['embedding'].values)\n",
        "\n",
        "# === Mapping case_id → pasal ===\n",
        "case_solutions = dict(zip(df[\"case_id\"], df[\"pasal\"]))\n",
        "\n",
        "# === RETRIEVAL ===\n",
        "def retrieve(query: str, k: int = 5, mode: str = 'tfidf') -> list:\n",
        "    if mode == 'bert':\n",
        "        q_vec = bert_embed(query)\n",
        "        sims = cosine_similarity(q_vec, X_embed)[0]\n",
        "    else:\n",
        "        q_vec = vectorizer.transform([query])\n",
        "        sims = cosine_similarity(q_vec, X_tfidf).flatten()\n",
        "\n",
        "    topk_idx = sims.argsort()[-k:][::-1]\n",
        "    return df.iloc[topk_idx]['case_id'].astype(str).tolist()\n",
        "\n",
        "# === PREDIKSI ===\n",
        "def predict_outcome(query, k=5, mode='tfidf', weighted=True):\n",
        "    top_k_ids = retrieve(query, k=k, mode=mode)\n",
        "\n",
        "    if not weighted:\n",
        "        solusi_list = [case_solutions.get(cid, 'UNKNOWN') for cid in top_k_ids]\n",
        "        predicted = Counter(solusi_list).most_common(1)[0][0]\n",
        "    else:\n",
        "        if mode == 'bert':\n",
        "            q_vec = bert_embed(query)\n",
        "            sims = cosine_similarity(q_vec, X_embed)[0]\n",
        "        else:\n",
        "            q_vec = vectorizer.transform([query])\n",
        "            sims = cosine_similarity(q_vec, X_tfidf).flatten()\n",
        "\n",
        "        topk_idx = sims.argsort()[-k:][::-1]\n",
        "        top_k_ids = df.iloc[topk_idx]['case_id'].astype(str).tolist()\n",
        "        solusi_list = [case_solutions.get(cid, 'UNKNOWN') for cid in top_k_ids]\n",
        "        weights = sims[topk_idx]\n",
        "\n",
        "        score = {}\n",
        "        for sol, w in zip(solusi_list, weights):\n",
        "            score[sol] = score.get(sol, 0) + w\n",
        "        predicted = max(score.items(), key=lambda x: x[1])[0]\n",
        "\n",
        "    return predicted, top_k_ids\n",
        "\n",
        "# === LOAD QUERIES & KONVERSI GROUND TRUTH ===\n",
        "with open(\"/content/drive/MyDrive/ProyekA/data/eval/queries.json\", encoding=\"utf-8\") as f:\n",
        "    queries = json.load(f)\n",
        "\n",
        "# Pastikan ground_truth diubah dari case_id ke pasal yang cocok\n",
        "for q in queries:\n",
        "    cid = str(q['ground_truth']).zfill(3)\n",
        "    q['ground_truth'] = case_solutions.get(cid, \"UNKNOWN\")\n",
        "\n",
        "# === JALANKAN PREDIKSI ===\n",
        "results = []\n",
        "for q in queries:\n",
        "    pred, top_k = predict_outcome(q[\"query_text\"], k=5, mode='bert', weighted=True)\n",
        "    results.append({\n",
        "        \"query_id\": q[\"query_id\"],\n",
        "        \"predicted_solution\": pred,\n",
        "        \"top_5_case_ids\": top_k,\n",
        "        \"ground_truth\": q[\"ground_truth\"],\n",
        "        \"query_text\": q[\"query_text\"]\n",
        "    })\n",
        "\n",
        "# === SIMPAN HASIL ===\n",
        "os.makedirs(\"/content/drive/MyDrive/ProyekA/data/results\", exist_ok=True)\n",
        "pd.DataFrame(results).to_csv(\"/content/drive/MyDrive/ProyekA/data/results/predictions.csv\", index=False)\n",
        "print(\"✅ Hasil prediksi berhasil disimpan ke predictions.csv\")\n"
      ]
    }
  ]
}